"""
Tests for Answer Builder (Hybrid LLM + Facts)
==============================================

Tests the hybrid answer generation approach:
- Deterministic fact extraction
- LLM rephrasing with safety constraints
- Fallback behavior when LLM fails

Related files:
- app/answer/answer_builder.py: Module under test
- app/dsl/schema.py: MetricQuery, MetricResult structures
- app/services/qa_service.py: Uses AnswerBuilder with fallback
"""

import pytest
from unittest.mock import Mock, MagicMock, patch

from app.answer.answer_builder import AnswerBuilder, AnswerBuilderError
from app.dsl.schema import MetricQuery, MetricResult, QueryType, Filters


# ============================================================================
# Fixtures
# ============================================================================

@pytest.fixture
def mock_openai_client():
    """
    Create a mock OpenAI client for testing.
    
    WHY mock:
    - Tests run without API key
    - Deterministic output (no LLM variance)
    - Fast tests (no network calls)
    
    Returns:
        Mock client with controllable responses
    """
    mock_client = MagicMock()
    
    # Default successful response
    mock_response = Mock()
    mock_choice = Mock()
    mock_message = Mock()
    mock_message.content = "This is a natural answer generated by GPT."
    mock_choice.message = mock_message
    mock_response.choices = [mock_choice]
    
    mock_client.chat.completions.create.return_value = mock_response
    
    return mock_client


@pytest.fixture
def answer_builder(mock_openai_client):
    """
    Create AnswerBuilder with mocked OpenAI client.
    
    WHY fixture:
    - Reuse across tests
    - Consistent setup
    - Easy to override if needed
    """
    builder = AnswerBuilder()
    builder.client = mock_openai_client
    return builder


# ============================================================================
# Fact Extraction Tests
# ============================================================================

def test_extract_metrics_facts_with_all_data(answer_builder):
    """
    Test fact extraction from complete metrics result.
    
    Ensures deterministic extraction of:
    - Metric value
    - Previous value
    - Change percentage
    - Top performer
    """
    dsl = MetricQuery(
        query_type=QueryType.METRICS,
        metric="roas"
    )
    
    result = MetricResult(
        summary=2.45,
        previous=2.06,
        delta_pct=0.189,
        breakdown=[
            {"label": "Summer Sale", "value": 3.20},
            {"label": "Winter Campaign", "value": 2.80}
        ]
    )
    
    facts = answer_builder._extract_metrics_facts(dsl, result)
    
    assert facts["metric"] == "roas"
    assert facts["value"] == 2.45
    assert facts["previous_value"] == 2.06
    assert facts["change_percent"] == 18.9  # 0.189 * 100
    assert facts["top_performer"] == "Summer Sale"


def test_extract_metrics_facts_minimal_data(answer_builder):
    """
    Test fact extraction with only summary (no comparison or breakdown).
    
    Ensures graceful handling of missing optional fields.
    """
    dsl = MetricQuery(
        query_type=QueryType.METRICS,
        metric="spend"
    )
    
    result = MetricResult(summary=1250.50)
    
    facts = answer_builder._extract_metrics_facts(dsl, result)
    
    assert facts["metric"] == "spend"
    assert facts["value"] == 1250.50
    assert "previous_value" not in facts
    assert "change_percent" not in facts
    assert "top_performer" not in facts


def test_extract_providers_facts(answer_builder):
    """
    Test fact extraction from providers query.
    
    Ensures:
    - Platform names are capitalized
    - Count is correct
    """
    result = {"providers": ["google", "meta", "tiktok"]}
    
    facts = answer_builder._extract_providers_facts(result)
    
    assert facts["platforms"] == ["Google", "Meta", "Tiktok"]
    assert facts["count"] == 3


def test_extract_entities_facts(answer_builder):
    """
    Test fact extraction from entities query.
    
    Ensures:
    - Entity type is pluralized
    - Entity names extracted
    - Status filter included if present
    """
    dsl = MetricQuery(
        query_type=QueryType.ENTITIES,
        filters=Filters(level="campaign", status="active")
    )
    
    result = {
        "entities": [
            {"name": "Summer Sale", "status": "active", "level": "campaign"},
            {"name": "Winter Promo", "status": "active", "level": "campaign"}
        ]
    }
    
    facts = answer_builder._extract_entities_facts(dsl, result)
    
    assert facts["entity_type"] == "campaigns"
    assert facts["entity_names"] == ["Summer Sale", "Winter Promo"]
    assert facts["count"] == 2
    assert facts["status"] == "active"


# ============================================================================
# Answer Generation Tests
# ============================================================================

def test_build_answer_success(answer_builder, mock_openai_client):
    """
    Test successful answer generation with LLM.
    
    Verifies:
    - LLM is called with correct prompt
    - Answer text is returned
    - Latency is measured (if requested)
    """
    dsl = MetricQuery(
        query_type=QueryType.METRICS,
        metric="roas"
    )
    result = MetricResult(summary=2.45)
    
    answer, latency = answer_builder.build_answer(dsl, result, log_latency=True)
    
    assert answer == "This is a natural answer generated by GPT."
    assert isinstance(latency, int)
    assert latency >= 0
    
    # Verify LLM was called
    assert mock_openai_client.chat.completions.create.called


def test_build_answer_llm_failure_raises_error(answer_builder, mock_openai_client):
    """
    Test that LLM failures raise AnswerBuilderError.
    
    WHY important:
    - QAService needs to catch this for fallback
    - Error message should be helpful for debugging
    """
    # Simulate LLM API failure
    mock_openai_client.chat.completions.create.side_effect = Exception("API Error")
    
    dsl = MetricQuery(
        query_type=QueryType.METRICS,
        metric="spend"
    )
    result = MetricResult(summary=1000.0)
    
    with pytest.raises(AnswerBuilderError) as exc_info:
        answer_builder.build_answer(dsl, result)
    
    assert "Answer generation failed" in str(exc_info.value)
    assert exc_info.value.original_error is not None


def test_build_answer_providers_query(answer_builder):
    """
    Test answer generation for providers query.
    
    Ensures providers facts are extracted and passed to LLM correctly.
    """
    dsl = MetricQuery(query_type=QueryType.PROVIDERS)
    result = {"providers": ["google", "meta"]}
    
    answer, _ = answer_builder.build_answer(dsl, result, log_latency=True)
    
    assert answer is not None
    # Verify the prompt contained the right facts
    call_args = answer_builder.client.chat.completions.create.call_args
    user_message = call_args[1]["messages"][1]["content"]
    assert "providers" in user_message.lower()


def test_build_answer_entities_query(answer_builder):
    """
    Test answer generation for entities query.
    
    Ensures entity facts are extracted and passed to LLM correctly.
    """
    dsl = MetricQuery(
        query_type=QueryType.ENTITIES,
        filters=Filters(level="campaign", status="active")
    )
    result = {
        "entities": [
            {"name": "Campaign A", "status": "active", "level": "campaign"}
        ]
    }
    
    answer, _ = answer_builder.build_answer(dsl, result, log_latency=True)
    
    assert answer is not None
    # Verify the prompt contained entity facts
    call_args = answer_builder.client.chat.completions.create.call_args
    user_message = call_args[1]["messages"][1]["content"]
    assert "entities" in user_message.lower()


# ============================================================================
# Prompt Construction Tests
# ============================================================================

def test_system_prompt_has_safety_instructions(answer_builder):
    """
    Test that system prompt includes critical safety rules.
    
    WHY critical:
    - Must prevent number invention
    - Must ensure facts-only usage
    """
    system_prompt = answer_builder._build_system_prompt()
    
    # Check for key safety instructions
    assert "do not invent" in system_prompt.lower()
    assert "only" in system_prompt.lower()
    assert "provided" in system_prompt.lower()
    
    # Check for tone guidance
    assert "natural" in system_prompt.lower() or "conversational" in system_prompt.lower()


def test_user_prompt_contains_facts(answer_builder):
    """
    Test that user prompt includes extracted facts.
    
    Ensures LLM receives all necessary information for rephrasing.
    """
    dsl = MetricQuery(
        query_type=QueryType.METRICS,
        metric="roas"
    )
    facts = {
        "metric": "roas",
        "value": 2.45,
        "change_percent": 19.0
    }
    
    user_prompt = answer_builder._build_user_prompt(dsl, facts)
    
    assert "roas" in user_prompt.lower()
    assert "2.45" in user_prompt
    assert "19" in user_prompt


# ============================================================================
# Integration Test
# ============================================================================

def test_full_pipeline_with_real_structures():
    """
    Integration test with real DSL and result structures.
    
    Uses mock LLM but real Pydantic models to ensure compatibility.
    
    WHY important:
    - Catches type mismatches
    - Validates end-to-end flow
    - Ensures all query types work
    """
    # Create builder with mock client
    builder = AnswerBuilder()
    mock_client = MagicMock()
    mock_response = Mock()
    mock_choice = Mock()
    mock_message = Mock()
    mock_message.content = "Natural answer from LLM"
    mock_choice.message = mock_message
    mock_response.choices = [mock_choice]
    mock_client.chat.completions.create.return_value = mock_response
    builder.client = mock_client
    
    # Test metrics query
    dsl_metrics = MetricQuery(
        query_type=QueryType.METRICS,
        metric="roas"
    )
    result_metrics = MetricResult(summary=2.45, delta_pct=0.19)
    answer1, _ = builder.build_answer(dsl_metrics, result_metrics)
    assert answer1 == "Natural answer from LLM"
    
    # Test providers query
    dsl_providers = MetricQuery(query_type=QueryType.PROVIDERS)
    result_providers = {"providers": ["google", "meta"]}
    answer2, _ = builder.build_answer(dsl_providers, result_providers)
    assert answer2 == "Natural answer from LLM"
    
    # Test entities query
    dsl_entities = MetricQuery(
        query_type=QueryType.ENTITIES,
        filters=Filters(level="campaign")
    )
    result_entities = {
        "entities": [{"name": "Campaign A", "status": "active", "level": "campaign"}]
    }
    answer3, _ = builder.build_answer(dsl_entities, result_entities)
    assert answer3 == "Natural answer from LLM"


# ============================================================================
# DSL v2.0.1 Tests: Rich Context Integration
# ============================================================================

class TestAnswerBuilderV201:
    """Tests for v2.0.1 rich context integration."""
    
    def test_build_answer_with_rich_context(self, answer_builder, mock_openai_client):
        """
        WHAT: Test answer generation with rich context
        WHY: Verifies context_extractor integration for metrics queries
        """
        # Setup mock response
        mock_openai_client.chat.completions.create.return_value.choices[0].message.content = (
            "Your ROAS jumped to 2.45× last week—19% higher than before. "
            "This is slightly above your workspace average of 2.30×."
        )
        
        dsl = MetricQuery(
            query_type=QueryType.METRICS,
            metric="roas"
        )
        
        result = MetricResult(
            summary=2.45,
            previous=2.06,
            delta_pct=0.189,
            timeseries=[],
            breakdown=[],
            workspace_avg=2.30  # NEW in v2.0.1
        )
        
        answer, latency = answer_builder.build_answer(dsl, result, log_latency=True)
        
        assert "2.45×" in answer
        assert len(answer) > 50  # Not just a number, but a full sentence
        assert isinstance(latency, int)
        
        # Verify the LLM was called with ANSWER_GENERATION_PROMPT (v2.0.1)
        call_args = mock_openai_client.chat.completions.create.call_args
        system_message = call_args[1]["messages"][0]["content"]
        assert "marketing analytics assistant" in system_message.lower()
    
    def test_build_answer_includes_workspace_comparison(self, answer_builder, mock_openai_client):
        """
        WHAT: Test that workspace_avg is included in context
        WHY: Ensures workspace comparison feature works
        """
        dsl = MetricQuery(
            query_type=QueryType.METRICS,
            metric="cpc"
        )
        
        result = MetricResult(
            summary=0.48,
            previous=0.55,
            delta_pct=-0.127,
            workspace_avg=0.52  # Above average (lower is better for CPC)
        )
        
        answer, _ = answer_builder.build_answer(dsl, result)
        
        # Verify LLM received workspace comparison context
        call_args = mock_openai_client.chat.completions.create.call_args
        user_message = call_args[1]["messages"][1]["content"]
        
        # Check that context includes workspace_comparison
        assert "workspace_comparison" in user_message or "workspace_avg" in user_message
    
    def test_build_answer_with_trend_data(self, answer_builder, mock_openai_client):
        """
        WHAT: Test that timeseries data is used for trend analysis
        WHY: Verifies trend extraction integration
        """
        dsl = MetricQuery(
            query_type=QueryType.METRICS,
            metric="roas"
        )
        
        result = MetricResult(
            summary=2.45,
            previous=2.06,
            delta_pct=0.189,
            timeseries=[
                {"date": "2025-10-01", "value": 2.1},
                {"date": "2025-10-02", "value": 2.3},
                {"date": "2025-10-03", "value": 2.5},
                {"date": "2025-10-04", "value": 2.7},
                {"date": "2025-10-05", "value": 2.8}
            ],
            breakdown=[]
        )
        
        answer, _ = answer_builder.build_answer(dsl, result)
        
        # Verify LLM received trend context
        call_args = mock_openai_client.chat.completions.create.call_args
        user_message = call_args[1]["messages"][1]["content"]
        
        # Check that context includes trend
        assert "trend" in user_message.lower()
    
    def test_build_answer_with_top_performer(self, answer_builder, mock_openai_client):
        """
        WHAT: Test that breakdown data includes top performer
        WHY: Verifies top_performer extraction integration
        """
        dsl = MetricQuery(
            query_type=QueryType.METRICS,
            metric="roas"
        )
        
        result = MetricResult(
            summary=2.45,
            previous=None,
            delta_pct=None,
            timeseries=[],
            breakdown=[
                {"label": "Summer Sale", "value": 3.2},
                {"label": "Winter Promo", "value": 2.8},
                {"label": "Spring Launch", "value": 1.9}
            ]
        )
        
        answer, _ = answer_builder.build_answer(dsl, result)
        
        # Verify LLM received top_performer context
        call_args = mock_openai_client.chat.completions.create.call_args
        user_message = call_args[1]["messages"][1]["content"]
        
        # Check that context includes top_performer
        assert "top_performer" in user_message.lower()
        assert "Summer Sale" in user_message
    
    def test_build_answer_performance_level_in_context(self, answer_builder, mock_openai_client):
        """
        WHAT: Test that performance_level is included for tone guidance
        WHY: Ensures GPT receives tone instructions
        """
        dsl = MetricQuery(
            query_type=QueryType.METRICS,
            metric="roas"
        )
        
        result = MetricResult(
            summary=3.5,
            previous=2.0,
            delta_pct=0.75,
            workspace_avg=2.0  # Way above average = EXCELLENT
        )
        
        answer, _ = answer_builder.build_answer(dsl, result)
        
        # Verify LLM received performance_level
        call_args = mock_openai_client.chat.completions.create.call_args
        user_message = call_args[1]["messages"][1]["content"]
        
        # Check that context includes performance_level
        assert "performance_level" in user_message.lower()
    
    def test_build_fallback_still_works(self):
        """
        WHAT: Test fallback answer generation (no LLM)
        WHY: Ensures fallback remains functional for error cases
        """
        from app.answer.answer_builder import AnswerBuilder
        
        builder = AnswerBuilder()
        
        dsl = MetricQuery(
            query_type=QueryType.METRICS,
            metric="roas"
        )
        
        result = MetricResult(
            summary=2.45,
            previous=2.06,
            delta_pct=0.189,
            timeseries=[],
            breakdown=[
                {"label": "Summer Sale", "value": 3.20}
            ],
            workspace_avg=2.30
        )
        
        # Call build_fallback directly (legacy method for error cases)
        answer = builder.build_fallback(result, dsl)
        
        # Verify fallback answer contains key information
        assert "2.45" in answer or "2.45×" in answer
        assert "+18.9%" in answer
        assert "Summer Sale" in answer

